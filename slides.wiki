== Introduction ==

==== Outline ====

\tableofcontents[currentsection]

==== \texttt{whoami} ====

* Valentin Haenel
* V.P. Engineering @ TwentyBN
* We have been training from videos for almost 24 months now

<[center]
    <<<images/20bn-logo.eps, scale=0.7>>>
[center]>

==== The Problem ====

<[block]{The Starving GPUs Problem}
Getting GPUs fully utilized when training neural networks from large-scale
video data can be challanging.
[block]>

==== The Reason(s) ====

* PCIe Bus
* Storage medium
* Data storage format and loader implementation

==== PCIe Bus ====

* Peripheral Component Interconnect Express
* Connects CPU and GPU
* Bandwidth (host-to-device) 16 GB/s (theoretical) 11 GB/s (measured)
* Not much we can do about it, except to throw money at the problem with NVLink

====  Storage Medium ====

* This only applies if the data doesn't fit into memory (or filesystem cache)
* Various technologies: magnetic disks and solid state disks
* Various connectors SATA III / PCIe | U.2 | M.2
* Bandwidth (one-way): 600 MB/s | 4 GB/s
* Parallelization with RAID
* Again: throw money at the problem

====  Data storage format and loader implementation ====

* Data storage format: how are the videos stored on disk?
* Data loader implementation: how are the videos loaded during training?
* This is all software and being smart here can have a high impact

== Data Formats ==

==== Outline ====

\tableofcontents[currentsection]

==== Data Formats: Bursted Frames or Encoded Videos ====

* Insight: A video is just a series of images
* Naive approach: decode the video into frames and store them on disk
* Slightly better: store the frames one after another in a binary file
* Maybe better:

==== GulpIO ====

* A simple binary storage format
* Essence: concatenated JPEGs on disk

<[center]
    <<<images/data_file_layout.pdf, scale=0.20>>>
[center]>

==== GulpIO ====

* Initial implementation for Kinetics last year, helped us get 3rd place

* https://github.com/TwentyBN/GulpIO
* https://medium.com/twentybn/introducing-gulpio-f97b07f1da58

* We are receiving pull-requests --> people are using it

==== Encoded Videos ====

* Principle: a video codec uses delta-compression and key frames. I.e. only \
  certain frames are stored and the following frames are stored as a difference \
  to these frames. \

* Issue: Video Codecs might be lossy and optimized for subjective human video \
  quality not for training

==== Encoded Videos ====

We have experience with:

* \texttt{mp4} as container and \texttt{h264} as codec
* \texttt{webm} as container and \texttt{vp9} as codec

==== Encoded Videos ====[containsverbatim]

* Assumption: smaller file size is better:

<[verbatim]
% ls -sh1 large_326ee10517267ff6e5be.*
1,3M video.mp4
400K video.webm
[verbatim]>

* But what about decoding?

==== Frames vs. Videos ====[containsverbatim]

<[verbatim]
% ffmpeg -i video.mp4 -q:v 1 -f image2
  -r 15 -vf 'scale=512x512' 'video_%04d.jpg' 2> /dev/null
% du -sch video_0*.jpg | tail -1
5,0M	total
[verbatim]>

* Obviously depends on both frame rate (\texttt{15}) and resolution (\texttt{512x512})

==== Optimizing Data for Loading ====

Regardless of weather you choose bursted frames or videos, you can tweak
frame-rate and resolution before training.

== Data Loaders ==

==== Outline ====

\tableofcontents[currentsection]

==== Open-Source Packages for Loading Videos ====

* All of the following use \texttt{ffmpeg} and/or the underlying library \texttt{av}

* scikit-video
* moviepy
* lintel
* nvvl
* PyAV

==== scikit-video ====

* Motto: \textit{Video Processing in Python}
* http://www.scikit-video.org/stable/
* Video loader inspired by \texttt{imageio}
* However: uses \texttt{popen} to execute either \texttt{ffmpeg} or \texttt{avconv}
* The result is a \texttt{fork} system call and multiple memory copies
* --> slow

==== MoviePY ====

* Motto: \textit{Video Editing in Python}
* http://zulko.github.io/moviepy/
* suffers from the same implementation weakness as scikit-video
* --> slow

==== Lintel ====

* Motto: \textit{A Python module to decode video frames directly, using the FFmpeg C API.}
* https://github.com/dukebw/lintel
* Fairly fresh library focussed on machine (deep) learning applications
* Avoids the issues of \texttt{imamgeio}, \texttt{scikit-video} and \texttt{moviepy} (even mentions this in the readme)
* However: segfaulted (and other issues) in the past
* Tricky to install as it requires compilation from source (also of the dependencies)

==== nvvl ====

* Motto: \textit{A library that uses hardware acceleration to load sequences of video frames to facilitate machine learning training}
* https://github.com/NVIDIA/nvvl
* Should in principle decode the video \textit{on the GPU}
* Theortically probably the smartest approach
* However: We could never get this to work :(

==== PyAV ====

* Motto: \textit{Pythonic bindings for FFmpeg}
* http://mikeboers.github.io/PyAV/
* Fully fledged bindings using \texttt{cython}
* Easy to install with \texttt{conda}
* Can convert directly to \texttt{numpy} arrays
* Anecdotal: We have seen memory leaks however...

==== Micro Benchmarks ====

* \texttt{scikit-video} vs. \texttt{PyAV} and \texttt{mp4/h264} vs.  * \texttt{webm/vp9}
* This will serve as an inidication of what we found in large scale trainings
* Micro-benchmarks are always bullshit, so I encourage you to challange me and * run your own

== Benchmarking ==

==== Outline ====

\tableofcontents[currentsection]

==== Interlude: \texttt{nocache} ====

* As stated earlier, we are assuming, that the dataset doesn't fit into memory and file-system cache.
* In order to simulate this during benchmarking, we use a tool called \texttt{nocache}
* This hijacks all systems calls to the file-system cache and thus the video is loaded from the storage medium each time

==== Experimental Setup ====[containsverbatim]

* Azure instance:
* 24 CPU cores
* All Software installed using \texttt{miniconda}
<[verbatim]
av                        0.3.3                    py36_2    conda-forge
sk-video                  1.1.10                     py_3    conda-forge
ffmpeg                    3.4.2                         0    conda-forge
[verbatim]>

==== Without \texttt{nocache} ====[containsverbatim]

<[nowiki]
\begin{Verbatim}[fontsize=\scriptsize]
In [3]: %timeit videodata = skvideo.io.vread('video.mp4')
331 ms ± 28.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [4]: %timeit reader = skvideo.io.vreader('video.mp4') ; [img for img in reader]
275 ms ± 17.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [5]: %timeit r = av.open('video.mp4') ; imgs = [f.to_rgb().to_nd_array() for f in r.decode(video=0)]
428 ms ± 3.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [6]: %timeit videodata = skvideo.io.vread('video.webm')
461 ms ± 23.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [7]: %timeit reader = skvideo.io.vreader('video.webm') ; [img for img in reader]
453 ms ± 12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [8]: %timeit r = av.open('video.webm') ; imgs = [f.to_rgb().to_nd_array() for f in r.decode(video=0)]
316 ms ± 4.52 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
\end{Verbatim}
[nowiki]>

==== With \texttt{nocache} ====[containsverbatim]

<[nowiki]
\begin{Verbatim}[fontsize=\scriptsize]
In [3]: %timeit videodata = skvideo.io.vread('video.mp4')
2.15 s ± 24 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [4]: %timeit reader = skvideo.io.vreader('video.mp4') ; [img for img in reader]
2.08 s ± 31.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [5]: %timeit r = av.open('video.mp4') ; imgs = [f.to_rgb().to_nd_array() for f in r.decode(video=0)]
424 ms ± 3.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [6]: %timeit videodata = skvideo.io.vread('video.webm')
3.75 s ± 72.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [7]: %timeit reader = skvideo.io.vreader('video.webm') ; [img for img in reader]
3.72 s ± 42.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [8]: %timeit r = av.open('video.webm') ; imgs = [f.to_rgb().to_nd_array() for f in r.decode(video=0)]
316 ms ± 4.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
\end{Verbatim}
[nowiki]>

==== Benchmarking during training ====[containsverbatim]

* Here is something our Principle A.I. Researcher sent me
* Average video loading time

<[verbatim]
    |Dataset | Num workers = 1 | Num workers = 10 |
    | ------ |:---------------:|:----------------:|
    |SkVideo |     110.4 ms    |       12.7 ms    |
    |PyAV    |     33.5 ms     |        5.3 ms    |
[verbatim]>

== Conclusion ==

==== Outline ====

\tableofcontents[currentsection]

==== Take-Home message ====

* If your goal is to have the GPUs fully utilized:
* \texttt{webm/vp9} is currently superior to \texttt{mp4/h264}
* \texttt{PyAV} is currently the superior loader
* Transcode your videos to the desired spatial and temporal resolution
* All of the above may change at any time
* Run your own benchmarks

==== A shameless plug ====

* Last week at TwentyBN we released version 2 of our flagship dataset: \texttt{something-something}
* https://20bn.com/datasets/something-something
* Free for academic use, licences available for commercial use
* We also released code to train some baselines under MIT licence
* https://github.com/TwentyBN/something-something-v2-baseline

\hspace{0.5cm}

* Yes the files are in \texttt{webm/vp9} format and yes there is a \texttt{PyAV} based loader

* Contact: \texttt{v@20bn.com}

\hspace{0.5cm}

* Questions?
